\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{authblk}
\usepackage{float}
\usepackage{rotating}
\usepackage{url}
\usepackage{lscape}
\usepackage{longtable}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{lineno}
\usepackage{amsmath,amsthm}
%\usepackage{fullpage}	
\usepackage{anysize}
\marginsize{1.0in}{1.0in}{1.0in}{1.0in}
\linespread{1.6}

\newcommand{\Pic}[2][0.85]{\begin{center}\includegraphics[width=0.8\textwidth,height=#1\textheight,keepaspectratio]{#2}
 \end{center} }


\title{Digital Elevation Models (DEMs) clustering for terrain modeling}
\author[1]{ E. R. Stefanescu }
\author[1]{A.K. Patra}
\author[2]{M. Bursik}
\affil[1]{Department of Mechanical and Aerospace Engineering, University at Buffalo, Buffalo, NY 14260}
\affil[2]{Department of Geology, University at Buffalo, Buffalo, NY 14260 }

\date{\today}


\begin{document}
\linenumbers
\maketitle

\begin{abstract}
We consider the problem of Digital Elevation Models (DEMs) segmentation 
in homogeneous regions, aiming for identification of plateaux, ridges, small drainages,
straight front slopes, valleys, and crests, in order to create ensembles of DEMs. These 
are then used in a systematic hazard analysis, resulting in a complete and complex
hazard maps. In the paper we explore and implement a method 
for segmentation using clustering, that is required / needed when we want to construct a 
sparse representation of the DEM.  The method -- spectral clustering, is extensively and 
successfully used in image segmentation. It is a complex 
method that accounts for the spatial correlation of the 
elevation points and has the advantage that it can be used for almost any application 
where relationships between topographic features and other components of landscapes 
are to be assessed. Here, the method is adapted for the case in 
which each data point has associated range of geomorphometric measures. 
\end{abstract}

\section{Introduction}
Information about topography is necessary for landscape evaluation, erosion studies,
hydrology and geophysical modeling, natural hazard prevention, etc. The classic way to 
incorporate relief units into a landscape assessment is to delineate them during field survey
or using stereo aerial photographs. This approach is relatively time-consuming and the results
depend on the subjective decision of the interpreter.
Several methods for the creation of landform elements using elevation-derived attributes are
described in the literature. Commonly, these techniques developed regions of homogeneity based
on common attributes and then classified those regions (or groups of regions) as elements. The 
most widely used techniques are: self organizing map \citep{Koh1995}, watershed segmentation 
\citep{Najman1996}, support vector machine \citep{Gunn1997}, segmentation using heuristic rules 
and fuzzy logic \citep{Sonka1999}, fuzzy $K$-means classification \citep{Burrough2000} and object-based 
image analysis \citep{Carleer2005}. Many of these techniques have drawbacks, especially when the
 method relies heavily on hydrological information and  requires data-specific knowledge; also these 
 methods don't incorporate autocorrelation between the same attribute at two locations in their models.
Digital Elevation Models (DEMs) are digital representations of terrain, and are represented as an 
array of squared cells (data points/ pixels) with an elevation associated to each data point. They can 
have different resolutions (5m, 30m, 90m, 120m, etc) and can be obtained from various methods 
(photometry, radar interferometry, laser altimetry, etc.). Usually the size of a DEM varies from
tens to hundreds of kilometers which can lead to thousands to millions of grid points. 
 
The aim of this paper is to quantify the variation in the output of a computational flow model for 
block and ash flows, when the model inputs, including the elevation values represented in the DEM, are
uncertain or given as a range of possible values. Integrating these variations in the possible flows 
as a function of input uncertainties provides well-defined data on the probability of hazard at
specific locations, i.e., a hazard map \citep{Keith}.  In particular, the focus here is on assessing the 
influence of DEM uncertainties and propose an improved method of generating ensembles of DEMs.

In this context, we would like to implement a more complex model segmentation of a DEM of Mammoth 
Mountain to create non-overlapping groupings of homogeneous regions. Mammoth Mountain (Fig.~\ref{fig:fig1}) 
is a volcano located in California and it was chosen due to the ease of obtaining data sets. 
\begin{figure}[ht!]
\center
      \includegraphics[width=10cm,height=11cm,keepaspectratio]{figs/Topsar5.png}\\
  \caption{Hillshade plot of the Mammoth Mountain  }\label{fig:fig1}
\end{figure}

\section{Methodology}
Segmentation is a broad term, covering a wide variety of problems and techniques. 
%Each segmentation method attempts to obtain a compact representation of its data set using some form of model of similarity.
Segmentation methods are based on some data point or region similarity
in relation to their local neighborhood. A variety of different methods have been proposed 
for image segmentation such as boundary-based segmentation, region-based segmentation and pixel-labeling. 
One view of segmentation is that we are trying to determine which components of the data set naturally 
``belong together". This is a problem known as \textit{clustering}. Clustering is difficult for a number of reasons. 
Real-life data may contain clusters of varying size and shape, whose number is unknown in advance. Noise 
and outliners can further complicate the task by connecting separate clusters. 
Spectral clustering was firstly developed in the context of graph partitioning problems \citep{Donath1973}, 
where the problem is to partition a weighted graph into disjoint pieces, minimizing the sum of the weights of the edges 
linking the disjoint pieces. In the graph the nodes represent the grid points and arcs represent affinities (``couplings") 
between nearby grid points. The final cluster assignments of the dataset can be achieved by optimizing some clustering 
criteria defined on the graph. The criteria of some spectral clustering methods are to optimize some cut value on an 
undirected graph, such as normalized cut \citep{ShiMalik2000}, ratio cut \citep{Hagen1992}, and min--max cut \citep{Ding2001}. 

To be able to perform the segmentation of the DEM in homogeneous regions
we need to specify a range of geomorphometric measures which can be extracted
from the surface. We define a \textit{feature matrix} of DEM attributes, consisting of elevation and 
first and second derivatives of elevation (slope, profile curvature and tangential curvature).
Slope and curvature are easily extracted from a DEM within a Geographical Information System (GIS).

In the next sections basic methodology for generating ensembles of DEMs using segmentation is presented, with emphasis 
on segmentation using spectral clustering.  Subsequent sections summarize the TITAN2D flow simulation tool and its use 
in a systematic hazard analysis. The hazard analysis tool itself uses ensembles of TITAN2D simulations to construct statistical surrogate models of flow outcomes at different locations as a function of model inputs, such as flow volume and pile initial 
location. Sampling of these surrogates leads to the construction of effective hazard maps that reflect the range of uncertainty 
in the model inputs.


\subsection{Spectral Clustering}
A digital representation of a terrain surface is an approximation of reality and is often subject to significant 
error. The error is usually not known in terms of both magnitude and spatial distribution. There are, in fact 
large uncertainties associated with the construction of DEMs. DEM vendors generally provide users with a
measure of vertical accuracy in the form of the root mean squared error (RMSE) statistic. One key feature of 
the DEM grid points, which are spatial data, is the autocorrelation of observations in space.  Generally, spatial 
autocorrelation refers to the correlation between the same attribute at two locations. Observations in close 
spatial proximity tend to be more related than observations at larger distances or separation. Based on this
assumption spectral clustering is performed to identify homogenous regions within a DEM. %\citep{Ng_Jordan, Luxburg}.

Compared with traditional clustering algorithms, spectral clustering has some advantages: can stably detect 
non-convex patterns and linearly non-separable clusters \citep{Sakai2009}, and can obtain the globally optimal 
solutions in a continuous domain by eigendecomposition  \citep{Archip2005}. As a discriminative approach, they 
do not make assumptions about the global structure of data. 
Instead, local evidence on how likely two data points belong to the same class is first collected and a global decision 
is then made to divide all data points into disjunct sets according to some criterion. Often, such a criterion can be
interpreted in an embedding framework, where the grouping relationships among data points are preserved as much
as possible in a lower-dimensional representation. What makes spectral methods appealing is that their global-optima 
in the relaxed continuous domain are obtained by eigendecomposition. However, to get a discrete solution from eigenvectors
often requires solving another clustering problem, albeit in a lower-dimensional space. That is, eigenvectors are treated as 
geometrical coordinates of a point set. Unfortunately, when the number of grid points (denoted as $n$) is large, spectral 
clustering encounters a quadratic resource bottleneck in computing pairwise similarity between $n$ nodes and storing 
that large matrix. Moreover, the algorithm requires considerable computational time to find the smallest $k$ eigenvalues of 
a Laplacian matrix. Eigenvalues and eigenvectors are at the heart of spectral clustering algorithms, and in spite of their 
importance, existing eigensolvers do not scale well. Fast computation schemes for spectral clustering have been proposed 
by  different authors. They focus on the eigenvector computation of a graph Laplacian defined by a matrix of data similarities.
The Krylov subspace methods \citep{Mahadevan2008}, are iterative algorithms for finding leading eigencomponents of a sparse 
matrix, while \cite{Dhillon2007} assume the availability of the similarity matrix and propose a method that does not use eigenvectors.
\cite{Fowlkes2004} propose using the Nystr\"{o}m approximation to avoid calculating the whole similarity matrix. In this paper we are 
using a method proposed by \cite{Song2008}, which parallelize spectral clustering on distributed computers to address
resource bottlenecks of both memory use and computation time.

\subsubsection{Approach}
For a given data set $P = \{ p_1,\dots, p_n \in R^d \}$, spectral clustering finds a set of data clusters, $\{C_1,\dots, C_k \subset P\}$,
on the basis of spectral analysis of a similarity graph. Spectral clustering builds a weighted graph $G(V,E)$, where $V$ represents vertices and $E$, edges. 
We represent each elevation point as a node in the graph $G$ and the links between the adjacent data points will form the edges of the graph. Spectral clustering partitions data points into groups such that the members of a group are similar to each other and dissimilar to data points outside of the group. Given data points, an affinity matrix can be represented by a weighted adjacency matrix $W \in \mathcal{R}^{n\times n}$, where $w_{ij}$ is a measure
of the similarity between grid point $i$ and grid point $j$. The affinity matrix is used to preserve the local structure of the patterns. It expresses the degree of similarity between points, and
it must have the following properties: i) non-negative; ii) symmetric; iii) invertible.
We have chosen the heat kernel for calculating the affinity matrix, as:
%\begin{align}
%W_{i,j} = \exp ( -  \frac{\lVert x_i - x_j \rVert^2}{\sigma^2} )
\begin{equation}
{\mathbf W_{ij}} = \left\{
\begin{array}{rl}
\exp{\frac{- \parallel F(i) - F(j) \parallel}{\sigma_F^2}}*exp{\frac{- \parallel x(i) - x(j) \parallel}{\sigma_x^2}}, & if \parallel x(i) - x(j) \parallel \le r\\
0, & \qquad \text{otherwise},
\end{array} 
\right.
\label{eq:eq1}
\end{equation} 
where $F(i)$ represents the DEM feature vector for node $i$, and $x(i)$ represents the coordinate location of $i^{th}$ node.
$\sigma_F $ and $\sigma_x$ are positive tuning parameter that controls the decay of the affinity \citep{Tung2010}.
The graph partitioning can be interpreted as a minimization problem of an objective function. Common objective functions are
the ratio cut (Rcut), normalized cut (Ncut) and min--max cut (Mcut) expressed as:
\begin{align}
Rcut (C_1,\dots,C_k) = \sum_{l=1}^k \frac{(C_l, P \setminus C_l)}{card\; C_l} \\
Ncut C_1,\dots,C_k) = \sum_{l=1}^k \frac{(C_l, P \setminus C_l)}{cut (C_l, P)}
\end{align}
and
\begin{align}
Mcut (C_1,\dots,C_k) = \sum_{l=1}^k \frac{(C_l, P \setminus C_l)}{cut (C_l,C_l)} 
\end{align}
Here, $cut(X,Y)$ is the sum of the edge weights between $\forall p \in X$ and $\forall p \in Y$, $P\setminus C_l$ is the complement
of $C_l \subset P$, and $card\; C_l$ denotes the number of points in $C_l$.
The degree $d_i$ of node $i$ is the sum of all edge weights incident on $x_i$:
\begin{align}
d_i = \sum_{j=1}^n w_{ij}
\end{align} 
The \textit{degree matrix D $\in \mathcal{R}^{n \times n}$} is defined as the diagonal matrix with the degrees $d_1, \dots, d_n$ on the diagonal, while the normalized graph Laplacian matrix $L \in \mathcal{R}^{n \times n}$ is defined as \citep{Chung1997, ShiMalik2000, Ng2002}:
\begin{align}
L = I - D^{-1/2}WD^{-1/2}
\end{align}
The data clustering by graph-cut boils down to the eigenvalue decomposition problem of $L$.
% followed by a K-means cluster in
%the eigenvector space. 
%$L$ satisfies the following properties:
%\begin{itemize}
%\item For every $f \in \mathcal{R}^n$ we have :
%\begin{equation}
%f' L f = \frac{1}{2} \sum_{i,j=1}^n w_{ij}(f_i - f_j)^2.
%\end{equation}
%\item $L$ is symmetric and positive semi-definite.
%\item The smallest eigenvalue of $L$ is $0$ and the corresponding eigenvector is the constant unity vector $1$.
%\item $L$ has $n$ non-negative, real-valued eigenvalues $0=\lambda_1 \le \lambda_2 \le \cdots \le \lambda_n$.
%\end{itemize}
%The most interesting aspect of spectral clustering is the mapping of the data points into a new space with
%$k$-dimensions by means of eigenvector decomposition. 
These eigenvectors induce an embedding of the data points in a low-dimensional subspace wherein a
partitioning based on the normalized cut (NCut) can be used. The solution of the minimization problem can be 
obtained from the Fiedler eigenvector \citep{Stella2003}.
 The steps involved in DEM segmentation using spectral clustering are summarized in Figure ~\ref{fig:fig2}.
\begin{figure}[ht!]
\center
      \includegraphics[width=10cm,height=11cm,keepaspectratio]{figs/Spectral_flow.pdf}\\
  \caption{Spectral clustering for DEM segmentation workflow}\label{fig:fig2}
\end{figure}
 
%In Figure~\ref{fig:fig3} the Spectral Clustering was performed for $\sigma=1$ and a DEM of 120m resolution. 
%\begin{figure}[ht!]
%\center
%      \includegraphics[width=10cm,height=11cm,keepaspectratio]{Spectral1.pdf}\\
%  \caption{Spectral Clustering $\sigma =1 $  }\label{fig:fig3}
%\end{figure}
%\begin{figure}[ht!]
%  \begin{minipage}[b]{0.5\textwidth}
%    \begin{tabular}{c}
%      \includegraphics[width=7cm,height=8cm,keepaspectratio]{pics/T5_K4.pdf}\\
%      (a)
%    \end{tabular}
%  \end{minipage}
%  % \hfill
%  \begin{minipage}{0.5\textwidth}
%    \begin{tabular}{c}
%      \includegraphics[width=7cm,height=8cm,keepaspectratio]{pics/T5_K6.pdf}\\
%      (b)
%    \end{tabular}
%  \end{minipage}
%  \caption{a) Hillshade plot of the Mammoth Mountain b) 3D plot Mammoth Mountain }\label{fig:fig1}
%end{figure}
%As a next step in our project we will try to improve the way the affinity matrix is define in spectral clustering,
%such that both location and the feature matrix to be incorporated. Also, a Gaussian Mixture clustering will be
%implemented.

\subsubsection{Parallel implementation}
When the number of data points (n) is large, the computational complexity of spectral decompositions can reach $O(n^3)$ ($W$ dense). If the affinity matrix $W$ is define as in Equation ~\ref{eq:eq1} then its construction takes $\mathcal{O}(n^2d)$ flops, which 
can be also computationally intensive if the data cardinality $n$ or dimensionality $d$ is large.
\cite{Chen2011} investigate approaches for large-scale spectral clustering and propose a parallel implementation, which is also
adopted in this paper. The strategy to address the computational and memory difficulties involves distributing $n$ data instances 
onto $p$ distributed machine node. On each node, parallel spectral clustering (PSC) computes the similarities between local data and the whole set in a way that uses minimal disk I/O. Then PSC stores the eigenvector matrix on distributed nodes to reduce 
per-node memory use. Together with parallel eigensolver and $K$-means, PSC archives good speedup with large data sets.

%\subsubsection{$K$-means}
%Since $EM$ algorithm convergences to local maxima, initialization is very important. The initialization 
%of the Gaussian mixture parameters is done using $K$-means.
%$K$-means is essentially a sorting and binning procedure where the user determines the number 
%of bins / clusters to be used and the rules for sorting. The
%$K$-means procedure sorts a set of data points into a specified number of clusters based on the 
%feature vector for each data point [3].
%Comparison of the $K$-means algorithm with the $EM$ algorithm for Gaussian mixtures shows that they are
%very similar [6]. The $K$-means algorithm uniquely assigns each data point within a cluster whereas the$EM$ algorithm makes
%the assignment based on the posterior probabilities.
%The data set available is a 5m DEM, covering an area of $\approx 94 km^2$ which results in $\approx 3$ millions 
%data points. To speed up the computational time, the majority of the analysis was performed on a decimated 
%DEM having a 120m resolution and 6550 data points. 
%The following steps are implemented for the $K$-means algorithm:
%\begin{itemize}
%\item We choose $k$ = 4, 6 and 10 clusters from the DEM. There are $\mu_1, \mu_2, \cdots , \mu_k$ means 
%that we use for initialization.
%\item For each elevation point the nearest mean is calculated.
%\begin{align}
%c_{i} = \underset{\mu_j \in \{\mu_1, \mu_1, \cdots, \mu_k \}}{\operatorname{argmax}} (x_i - \mu_j)^2
%\end{align}
%\item The means values were updated based on the elevation points assigned to them. 
%\item The above steps were repeated until the locations of the means were no longer changing
%by a significant amount.
%\end{itemize}

\section{DEM ensembles}
When propagating uncertainty in DEMs through a geophysical system, stochastic methods are considered to 
be an effective way to estimate the probability density function of outputs by addressing uncertainties present in 
initial conditions and in model approximations.
In previous work \cite{stefanescuPRS2012} presented a basic methodology for generating
ensembles of DEMs representative of the true DEM. Here, we are extended the methodology at the cluster level, 
by making the assumption that each homogenous regions has its own error model which leads to different random 
fields. The random fields are used in creating multiple equally likely representations of an actual terrain
surface, following the approached suggested by \cite{Ehlschlaeger_1994}.
A normal distribution (mean of 0.0 and variance of 1.0) of maps or realizations is computed to reproduce the 
spatial autocorrelation encountered in the original error surface, filtered using a Gaussian convolution filter, with 
kernel sizes derived from autocorrelation analysis of the original error surfaces.
The random field function derives its spatial dependence from the use of a distance
based decay filter function. The following equation is used to
generate the random field:
\begin{align}
  &Z(\mathcal{U})= \frac{\sum_v w_{u,v}\epsilon_v}{\sqrt{\sum_v
      w_{u,v}^2}}, \quad u\in \mathcal{U}, \; v \in \mathcal{V}
 \label{eqn1}
 \end{align}
 \begin{align}
   w_{u,v} = \left\{ \begin{array}{ll} 1 & \quad :d_{u,v} \le F \\
       \left(1- \frac{d_{u,v} - F}{D - F} \right)^E & \quad F <
       d_{u,v} \le D, \; u \in \mathcal{U}, \; v \in \mathcal{V}\\ 0 &
       \quad :d_{u,v} > D
\end{array} \right.
\label{eqn2}                                                    
\end{align}
where $\mathcal{V}$ is the set of points potentially influencing
points in a given area, $\mathcal{U}$, $w_{u,v}$ is the spatial
autocorrelative effect between points $u \in \mathcal{U}$ and $v \in
\mathcal{V}$, $\epsilon_v$ is a Gaussian random variable with a mean of 0 and
variance of 1, $d_{u,v}$ is the distance between $u$ and $v$, $D$ is
the minimum distance of spatial independence, $E$ is the distance
decay exponent, and $F$ the distance at which errors are completely
correlated.

A set of random fields are created for each homogenous region/ cluster and are calibrated to 
the spatial variation of the field being simulated using a correlogram function. This is done by
fitting the correlogram and choosing the best descriptive parameters
of the random field (the minimum distance of spatial independence, the
correlated distance decay exponent and the filter parameter) in a
weighted least-square estimator.  After running hundreds of tests with
multiple combinations of $D$, $E$ and $F$, the best random field was 
found by fitting the error map characteristics such that the sum of
least squares difference between an error field's correlogram and the
target correlogram is minimized.

Each error realization was added
to the ``true'' DEM indicated as $m(\mathcal{U})$, to generate equally probable realizations of the
topography for the error structure of a DEM under consideration:
\begin{equation}
 R(\mathcal{U})=m(\mathcal{U})+m(m(\mathcal{T}))+(m(s^2(\mathcal{T}))\cdot \epsilon)\cdot Z(\mathcal{U})
\label{eq:one}
\end{equation} 
where $R(\mathcal{U})$ is a realization of the elevation dataset $m(\mathcal{U})$, $\mathcal{T}$ is a
group of sets of spatially uncorrelated sample points in $m(\mathcal{U})$, and $\epsilon$
is a Gaussian random variable with mean 0.0 and variance 1.0. $m(m(\mathcal{T}))$ and
variance $m(s^2(\mathcal{T})$ is mean and variance, respectively, of all sets in 
$\mathcal{T}$. $Z(\mathcal{U})$ specifies the random field as
defined in Equation ~\ref{eqn1} for each homogenous region. 
 \begin{figure}[H]\centering
\subfigure[]{\includegraphics[width=3in]{figs/Flow_map_179_noclust_legend.png}}
\subfigure[]{\includegraphics[width=3in]{figs/Flow_map_179_clust_legend.png}}
\subfigure[]{\includegraphics[width=3in]{figs/Flow_map_523_noclust_legend.png}}
\subfigure[]{\includegraphics[width=3in]{figs/Flow_map_523_clust_legend.png}}
\caption{Flow maps for two selected parameters values when DEM was created using a) no cluster b) cluster  }
\label{fig:rapido}
\end{figure} 

\section{Flow simulator and Hazard map construction}

\section{Results and Conclusions}
\subsection{Earth Mover's Distance}

%The data set available is a 5m DEM, covering an area of $\approx 94 km^2$ which results in $\approx 3$ millions 
%data points. To speed up the computational time, the majority of the analysis was performed on a decimated 
%DEM having a 120m resolution and 6550 data points. 
%
%The implementation of equation 11 it is still a work in progress. 
%We succeeded in performing the classic EM Gaussian Mixture (EMGM) on the \textit{feature matrix} 
%$X = \{x_k | 1\le k \le N\}$ with dimensionality $d$,
%where $x_k$ is a $d \times 1$ matrix. In this case $d=4$ and $N=6550$. 
%The feature were previously scaled such that their means were equal to zero.
%The initial parameters were chosen from the results of $K$-means results. The results are presented in Figure~\ref{fig:fig3} (b) and are compared to the Matlab build-in function 
%(Fig. ~\ref{fig:fig3} (a)).
%The results are quite different and we assume that using $K$-means for parameter initialization makes a significant difference in the clustering process.
%Also, adding more geomorphometric features results in differing assignments of data points to clusters.
%
%
%
%Spectral clustering was performed for $K=6$ and $K=20$ (Fig. ~\ref{fig:fig4} (a), (b)). We observe that classical EMGM clustering (Fig.~\ref{fig:fig3} (b)) and spectral clustering (Fig.~\ref{fig:fig4} (a)) give similar results which give us confidence of the \textit{goodness of fit} for those methods, considering that we had not used any performance measures. 
%Spectral clustering requires setting a set of parameters, namely, the scaling parameters $\sigma_F$ and $\sigma_x$. These parameters are very important since they add more weigh to the features that we consider that are more important than the others.
%Appropriate setting of $\sigma$ is crucial for obtaining good segmentation results in spectral clustering. Unfortunately, it is
%difficult to choose the appropriate $\sigma$ value, and it is always set manually. The incorrect value of  $\sigma$ can degrade the performance as the spectral clustering is highly sensitive to $\sigma$, and different values of 
%$\sigma$ may lead to drastically different results (Fig. ~\ref{fig:fig4} (c), (d)). The proper setting of the scaling parameter still remains an open issue, and there is no known effective method.
 
%\begin{figure}[ht!]
%  \begin{minipage}[b]{0.5\textwidth}
%    \begin{tabular}{c}
%      \includegraphics[width=8cm,height=7cm,keepaspectratio]{figs/Spectral1_old.pdf}\\
%      (a)
%    \end{tabular}
%  \end{minipage}
%  % \hfill
%  \begin{minipage}{0.5\textwidth}
%    \begin{tabular}{c}
%      \includegraphics[width=8cm,height=7cm,keepaspectratio]{figs/T30_K10.pdf}\\
%      (b)
%    \end{tabular}
%  \end{minipage}
%  \begin{minipage}[b]{0.5\textwidth}
%    \begin{tabular}{c}
%      \includegraphics[width=8cm,height=7cm,keepaspectratio]{figs/Spectral2.pdf}\\
%      (c)
%    \end{tabular}
%  \end{minipage}
%  % \hfill
%  \begin{minipage}{0.5\textwidth}
%    \begin{tabular}{c}
%      \includegraphics[width=8cm,height=7cm,keepaspectratio]{figs/Spectral1.pdf}\\
%      (d)
%    \end{tabular}
%  \end{minipage}
%   \caption{Spectral Clustering for a) K=6 -- one feature b) K=20 -- one feature c) K=6  where $\sigma_F = 0.8$ and $\sigma_x = 1$ -- 4 feature d) K=6  $\sigma_F = 0.8$ and $\sigma_x = 0.8$ -- 4 feature}\label{fig:fig4}
%\end{figure}

\begin{figure}[H]
\begin{centering}
 \includegraphics[width=8cm,height=7cm,keepaspectratio]{figs/Spectral_parallel.pdf}
 \caption{Parallel spectral clustering}
 \end{centering}
 \end{figure}

\begin{figure}[H]
  \begin{minipage}[b]{0.5\textwidth}
    \begin{tabular}{c}
      \includegraphics[width=8cm,height=7cm,keepaspectratio]{figs/hazard_map_noclusters.jpg}\\
      (a)
    \end{tabular}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.5\textwidth}
    \begin{tabular}{c}
      \includegraphics[width=8cm,height=7cm,keepaspectratio]{figs/hazard_map_clusters.jpg}\\
      (b)
    \end{tabular}
  \end{minipage}
  \caption{Hazard map a) no cluster b)clusters}\label{fig:fig5}
\end{figure}


\section*{Acknowledgment}
 
%
%\subsubsection*{References}
%\small{
%%[1] Ambroise, C., Dang, M. \& Govaert, G. (1996) Clustering of spatial
%%data by the EM algorithm. {\it Geostatistics for Environmental Applications},
%%pp. 493-504.
%
%[1] Junior, E.R.S., Cavalcanti, G.D.C. \& Ren, T.I. (2010) Does the affinity matrix
%influence the performance of the Locality Preserving Projection algorithm? 
%{\it IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, Istanbul. 
%
%[2] von Luxburg, U. (2007) A Tutorial on Spectral Clustering. {\it Statistics 
%and Computing} {\bf 17}.
%
%[3] MacQueen, J.B. Some methods for classification and analysis of multivariate 
%observations. {\it in Berkley Symposium on Mathematical Statistics and Probability},
%vol.1.
%
%[4] Ng, A.Y., Jordan, M.I., \& Weiss, Y. (2002) On Spectral Clustering: Analysis
%and an algorithm. {\it NISP} {\bf14}.
%
%[5] Archip, N., Rohling R., Cooperberg P., Tahmasebpour H., Warfield S.K. (2005) Spectral 
%Clustering Algorithms for Ultrasound Image Segmentation. 
%{\it Int Conf Med Image Comput Comput Assist Interv.} {\bf 8(Pt 2)}:862-9. PMID: 16686041
%
%[6]  Bishop, C. M.,(2007) Pattern Recognition and Machine Learning, {\it Springer}
%
%[7] Sujaritha, M., S., Annadurai (201) Color Image Segmentation using Adaptive Spatial
%Gaussian Mixture Model, {\it Int Journal of Information and Comm Eng.} {\bf 6:1}
%
%[8] Tung, F., A. Wong \& D. A., Clausi (2010) Enabling scalable spectral clustering for image
% segmentation {\it Pattern Recognition} {\bf 43}, 4069-4076
%
%[9] Xiangrong, Z., L., Jiao, L., Bo \& M., Gong Spectral clustering ensemble applied to SAR
% image segmentation.% {\it MS (w) GBIS (x) Consensus}
%}



\bibliographystyle{plainnat} \bibliography{mybib}
\end{document}