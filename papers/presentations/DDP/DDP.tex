\documentclass[10pt,mathserif]{beamer}

\usepackage{graphicx,amsmath,amssymb,tikz,psfrag}
%\usepackage{multimedia}
\usepackage{color}
%\AtBeginSection[] 
%{ 
%       \begin{frame}<beamer> 
%               \frametitle{Outline} 
%               \tableofcontents[currentsection,currentsubsection] 
%       \end{frame} 
%} 

%% begin presentation

%\title{\large \bfseries Deep Reinforcement Learning}

\title{\large \bfseries Differential Dynamic Programming}

\author{Ramona Stefanescu\\[3ex]
}
\date{\today}

\begin{document}

\frame{
\thispagestyle{empty}
\titlepage
}

\section{Introduction}

\begin{frame}{MDPs}
\begin{itemize}
\item A (discounted infinite horizon) Markov Decision Process (MDP) is a tuple (S,A,T, $\gamma$, D,R)
\item S is the set of possible states for the system
\item A is the set of possible actions
\item T represents the (typically stochastic) system dynamics
\item D is the initial-state distribution, from which state $s_0$ is drawn
\item $R:S\rightarrow R$ is the reward function 
\end{itemize}
Acting in a Markov decision process results in a sequence of states and actions $s_0$,$a_0$,$s_1$,$s_2,\dots$
\end{frame}

\begin{frame}
A policy $\pi$ is a sequence of mappings $(\mu_0, \mu_1,\mu_2 \dots)$, where, at time $t$ the mapping $\mu_t(\cdot)$ determines the action $a_t =\mu_t(s_t)$ to take when in state $s_t$ 
\begin{itemize}
\item The objective is to find policies that maximize the expected sum of rewards accumulated over time
\item In particular, a policy $\pi$ is good if its utility is high:
\begin{align}
U(\pi) = \mathbb{E}[\sum_{t=0}^{\infty}\gamma^tR(s_t) | \pi]
\end{align}
\item To represent the system dynamics, we can use the state-transition distribution notation
\begin{align}
s_{t+1} \propto P_{sa}(\cdot|s_t,a_t)
\end{align}
\item Or using a deterministic function $F$ and a random disturbance $\omega_t$
\begin{align}
s_{t+1} = F(s_t, a_t, \omega_t)
\end{align}
\end{itemize}
\end{frame}

\begin{frame}{Example - car}
\begin{itemize}
\item One way to model the state of a car is to use the following six state variables: northing ($n$),
easting ($e$), north velocity ($\dot{n}$), east velocity ($\dot{e}$), heading ($\theta$), angular rate ($\dot{\theta}$). Hence the state space $\mathbb{S}= \mathbb{R}^6$
\item The actions (or control inputs) are (i) steering angle, (ii) throttle, (iii) brake
\item The perturbances capture both environmental perturbations as well as unmodeled aspects of the car dynamics
\item We could have the followinh dynamics model $s_{t+1} = F(s_t, a_t, \omega_t)$:
\begin{align}
n_{t+1} &= n_t + \dot{n_t}\Delta_t,\\
e_{t+1} &= e_t + \dot{e_t}\Delta_t, \\
\theta_{n+1}&=\theta + \dot{\theta}\Delta_t\\
\dot{n}_{t+1} &= f_n(\dot{n}_t,\dot{e}_t,\dot{\theta}_t, a_t, \omega_t)\\ 
\dot{e}_{t+1} &= f_e(\dot{n}_t,\dot{e}_t,\dot{\theta}_t, a_t, \omega_t)\\
\dot{\theta}_{t+1} &= f_{\theta}(\dot{n}_t,\dot{e}_t,\dot{\theta}_t, a_t, \omega_t)
\end{align} 
\item The reward function could be $R(s_t) = 1$ (in goal region) and $100$(in collision)
\end{itemize}
\end{frame}

\begin{frame}{Finding optimal policies: value iteration}
The goal is to find an optimal policy - the policy that maximizes the expected total of rewards 
earned over the period of our decision process.
\begin{itemize}
\item \textbf{Finite horizon:} we are concerned with decisions and rewards only up until a given time $t=H$, with state space $S$ and action space $A$ finite. 
\item The value of any policy $\pi$ is:
\begin{align}
V_{\pi}(s_0) = \mathbb{E}[\sum_{t=0}^H \gamma^tR(s_t)|\pi; s_0]
\end{align}
we are interested in finding
\begin{align}
&\max_{\pi} V_{\pi}(s_0) \\
&\pi = \{\mu_0, \mu_1,\dots, \mu_{H-1}\}
\end{align}
where $\mu_i: S\rightarrow A$
\end{itemize}
\end{frame}

\begin{frame}{Finite horizon}
\begin{itemize}
\item Since there are $|A|^{|S|}$ possible mapping for each $\mu_i$, there are $(|A|^{|S|})^H$ possible policies $\pi$, which is far too many to compute the value of all possible options
\item Instead, we apply a dynamic programming algorithm known as value iteration to find the optimal policy efficiently
\item Intuitively, we are applying the notion that given a state, the past and future are independent (the "Markov property")
\item Define the value function at the $k$th time-step as
\begin{align}
V_k(s_k) = \max_{\mu_k, \mu_{k+1}, \dots, \mu_{H-1}} \mathbb{E}[\sum_{t=k}^H\gamma^{t-k}R(s_t)|s_k]
\end{align}
\item It can be shown that the optimal policy can be found by working backwards from $H$:
\begin{align}
V_k(s_k) = \max_{a\in A}[R(s_k) + \gamma\sum_{s'}P(s'|s,a)V_{k+1}(s')]
\end{align}
\end{itemize}
\end{frame}


\begin{frame}{Infinite horizon optimal policies}
\begin{itemize}
\item Now we are interested in finding an optimal policy when our horizon in infinite
\item The value of a policy is now
\begin{align}
V_{\pi}(s_0) = \mathbb{E}[\sum_{t=0}^{\infty}\gamma^tR(s_t)|s_0,\pi]
\end{align}
\item And we are interested in finding $V^*(s) = \max_{\pi}V_{\pi}(s)$ and $\pi^* = argmax_{\pi}V_{\pi}(s)$
\item Despite the fact the expectation is taken over an infinite sum, it is guaranteed to converge.
This is because $R$ is a function over a finite state space, and is therefore bounded, and since $\gamma \in(0,1)$
\end{itemize}
\end{frame}

\begin{frame}{Bellman Backup Operator}
\begin{itemize}
\item Let $V:S\rightarrow R$ be our value function and let $V_{\pi}(s) = \mathbb{E}[\sum_{t=0}^{\infty}\gamma^tR(s_t)|s_0,\pi]$.
\item Define the operator $T:V\leftarrow TV$ as
\begin{align}
(TV)(s) = \max_{a\in A}[R(s) + \gamma\sum_{s'}P(s'|s,a)V(s')]
\end{align}
\end{itemize}
\end{frame}

\begin{frame}{LQR - Finite Horizon Value Iteration Case Study}
\begin{itemize}
\item We assume a linear dynamic system:
\begin{align}
x_{t+1} = A_tx_t + B_tu_t,
\end{align}
where $x(t)$ denotes the state at time $t$ and $u(t)$ denotes the input at time $t$
\item We assume a quadratic cost function:
\begin{align}
J = \sum_{t=0}^{H-1}(x_t^TQ_tx_t + u_t^TR_tu_t) + x_H^TP_Hx_H
\end{align}
with for all $t$, $Q_t\ge 0, R_t\ge0$
\item Our goal is to find the input sequence $\{u_0, u_1,\dots, u_{H-1}\}$ that minimizes the cost: 
$\min_{u_0\dots u_{H-1}}J$
\end{itemize}
\end{frame}

\begin{frame}{LQR - cont}
\begin{align} \nonumber
&\min_{u_0\dots u_{H-1}}J \\ \nonumber
& = \min_{u_0\dots u_{H-1}} \sum_{t=0}^{H-1}(x_t^TQ_tx_t + u_t^TR_tu_t) + x_H^TP_Hx_H\\ \nonumber
& = \min_{u_0\dots u_{H-2}} \sum_{t=0}^{H-2}(x_t^TQ_tx_t + u_t^TR_tu_t) + x_{H-1}^TQ_{H-1}x_{H-1} + \\ \nonumber
&+\min_{u_{H-1}}u_{H-1}^TR_{H-1}u_{H-1} + x_{H}^TQ_Hx_H \nonumber
\end{align}
\end{frame}

\begin{frame}{LQR - cont}
Solving the equation the previous page leads to the following dynamic programming algorithm to find the optimal
controller for a linear system with quadratic costs:
\begin{itemize}
\item for $t= H-1$ to $0$ 
\item $\rightarrow$ $K_t \leftarrow -(R_t + B_t^TP_{t+1}B_t)^{-1}B_t^TP_{t+1}A_t$ 
\item $\rightarrow$ $P_t \leftarrow Q_t + K_t^TR_tK_t + (A_t + B_tK_t)^TP_{t+1}(A_t + B_tK_t)$
\item next $t$
\end{itemize}
The optimal inputs are computed as follows $u_t=K_tx_t$. The optimal cost-to-go (=cost incurred in all future
steps) for being in state $x_t$ at time $t$ is given by $x_t^TP_tx_t$.
\end{frame}


\begin{frame}{Controllability vs Observability}
\begin{itemize}
\item 
\end{itemize}
\end{frame}

\end{document}
